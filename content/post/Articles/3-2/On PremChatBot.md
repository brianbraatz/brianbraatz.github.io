---
title: On-Prem AI Chatbot for PDF Search
description: "Learn how to build a fully on-prem AI-powered chatbot "
slug: on-prem-ai-pdf-search
date: 2017-06-15
image: post/Articles/IMAGES/omnibot2000.png
categories:
  - AI
  - Machine Learning
  - PDF Search
tags:
  - AI
  - Machine
  - Learning
  - NLP
  - FAISS
  - Elasticsearch
  - PDF
  - Processing
draft: false
weight: 25
categories_ref:
  - AI
  - Machine Learning
  - PDF Search
slug_calculated: https://brianbraatz.github.io/p/on-prem-ai-pdf-search
lastmod: 2025-03-14T16:40:12.851Z
---
[Omnibot 2000 - Wikipedia](https://en.wikipedia.org/wiki/Omnibot)

<!-- for searching PDFs using PyMuPDF, FAISS, sentence-transformers, and Llama 2. In this first part, we cover text extraction using PyMuPDF and pdfplumber.

# Building an On-Prem AI Chatbot for PDF Search - Part 1: Text Extraction

Welcome to the first part of this **on-prem AI chatbot series**!  -->

This is a **simplified version** of a real-world project I built for a job.

The goal: **Create a fully on-prem AI chatbot** that can search and retrieve information from a **large collection of PDFs**.

Unlike cloud-based solutions, **everything runs locally**‚Äîwhich means **no API costs, no data privacy concerns, and full control** over the system.

***

## üõ†Ô∏è What We're Building

This guide will walk through setting up an **AI-powered PDF search system** using the following tools:

| Component           | Tool                              |
| ------------------- | --------------------------------- |
| **Text Extraction** | `PyMuPDF` (fastest)               |
| **Keyword Search**  | `Elasticsearch`                   |
| **Semantic Search** | `FAISS` (or `Qdrant`)             |
| **Embedding Model** | `sentence-transformers`           |
| **Chatbot LLM**     | `Llama 2` (running on local GPUs) |
| **User Interface**  | `Streamlit`                       |

## ‚ùó A Quick Note About OCR

For this project, **I didn‚Äôt need OCR** because all the PDFs already contained selectable text.

However, **if you‚Äôre dealing with scanned PDFs (images instead of text),** you‚Äôll need OCR (Optical Character Recognition).

For those cases, check out: [How to OCR PDFs using pdfplumber and Tesseract](https://github.com/jsvine/pdfplumber).

But for **this tutorial**, we‚Äôre assuming **text-based PDFs only**.

***

# üìù Part 1: Extracting Text from PDFs

Before we can **search** or **chat** with our PDFs, we need to **extract the text**.

The best way to do this **without OCR** is using `PyMuPDF` (`fitz`), which is **blazing fast** and maintains formatting.

## üì¶ Step 1: Install Dependencies

First, install PyMuPDF:

```bash
pip install pymupdf
```

## üöÄ Step 2: Extract Text from a PDF

Here‚Äôs a simple function to extract text from **any text-based PDF**:

```python
import fitz  # PyMuPDF

def extract_text_from_pdf(pdf_path):
    """Extracts text from a PDF using PyMuPDF."""
    doc = fitz.open(pdf_path)
    text = "\n".join([page.get_text("text") for page in doc])
    return text

# Example Usage
pdf_text = extract_text_from_pdf("example.pdf")
print(pdf_text[:500])  # Print first 500 characters
```

‚úÖ **Why PyMuPDF?**

* Super **fast** üöÄ
* Preserves **text structure**
* Can handle **large PDFs** without issues

‚ùå **When it won‚Äôt work**

* If the PDF is a scanned image, PyMuPDF **won't** extract anything
* If you get **empty text**, your PDF likely **needs OCR**

Again, if OCR is needed, check out: [How to OCR PDFs using pdfplumber and Tesseract](https://github.com/jsvine/pdfplumber).

***

## üî• Step 3: Batch Process a Folder of PDFs

If you have **hundreds or thousands of PDFs**, you‚Äôll want to **process them all at once**.

Here‚Äôs how to extract text from every PDF in a folder and store the results in a dictionary:

```python
import os

def extract_text_from_folder(pdf_folder):
    """Extracts text from all PDFs in a folder."""
    extracted_texts = {}
    
    for filename in os.listdir(pdf_folder):
        if filename.endswith(".pdf"):
            pdf_path = os.path.join(pdf_folder, filename)
            text = extract_text_from_pdf(pdf_path)
            extracted_texts[filename] = text
            
    return extracted_texts

# Example Usage
pdf_texts = extract_text_from_folder("pdf_documents")
print(pdf_texts.keys())  # Print the names of processed PDFs
```

### üîπ What This Does:

* Loops through all PDFs in a given folder
* Extracts text and stores it in a dictionary `{filename: extracted_text}`
* Can be used later for **search indexing**

***

## ‚úÖ What We Have So Far

At this point, we can **extract text from PDFs**, which is the **first step** toward building our AI-powered search system.

<!-- 
| Feature  | Status |
|----------|--------|
| **Text Extraction (Digital PDFs)** ‚úÖ Done |
| **OCR for Scanned PDFs** ‚ùå Not included (but linked) |
| **Batch Processing** ‚úÖ Done |

---

## üîú Coming in Part 2: Indexing and Searching PDFs  
Now that we have **extracted text**, the next step is **storing and searching it efficiently**.  

### **üîπ In Part 2, we‚Äôll cover:**  
‚úÖ Storing extracted text in **Elasticsearch** for keyword search  
‚úÖ Setting up **FAISS** for fast **semantic search**  

Stay tuned for **Part 2!** üöÄ  

---

## üóÇÔ∏è Key Takeaways  

| Feature | Tool |
|---------|------|
| **Digital PDF Extraction** | PyMuPDF |
| **Batch Processing PDFs** | Python + OS Module |
| **OCR for Scanned PDFs?** | Not included (but linked) |

---

## üìö References  
- [PyMuPDF Documentation](https://pymupdf.readthedocs.io/en/latest/)  
- [pdfplumber GitHub (For OCR)](https://github.com/jsvine/pdfplumber)  

---

‚úÖ **Next Up:** [On-Prem AI PDF Search - Part 2: Indexing with Elasticsearch & FAISS](#) (Coming Soon)  
```

---

### üîπ What's Different in This Version?
‚úÖ Clearly explains that this is based on **a real-world project**  
‚úÖ States **OCR is not included** (but links to an OCR guide)  
‚úÖ Focuses **only on extracting text from digital PDFs**  
‚úÖ Includes **batch processing** for efficiency   -->

# üî•Indexing PDFs in Elasticsearch (Keyword Search)

### üì¶ Step 1: Install Elasticsearch & Python Client

First, we need to install Elasticsearch and the Python client.

#### **Option 1: Run Elasticsearch Locally (Recommended)**

Install Elasticsearch (7.x or 8.x) from [elastic.co](https://www.elastic.co/downloads/elasticsearch), then start it:

```bash
./bin/elasticsearch
```

#### **Option 2: Run Elasticsearch via Docker**

```bash
docker run -d --name elasticsearch -p 9200:9200 -e "discovery.type=single-node" docker.elastic.co/elasticsearch/elasticsearch:8.5.0
```

Now, install the Python client:

```bash
pip install elasticsearch
```

***

### üöÄ Step 2: Index Extracted PDF Text

We'll store **each PDF's extracted text** as a document in Elasticsearch.

#### **1Ô∏è‚É£ Connect to Elasticsearch**

```python
from elasticsearch import Elasticsearch

es = Elasticsearch("http://localhost:9200")  # Change if running remotely

# Check connection
if es.ping():
    print("Connected to Elasticsearch!")
else:
    print("Elasticsearch connection failed.")
```

#### **2Ô∏è‚É£ Create an Index for PDFs**

```python
INDEX_NAME = "pdf_documents"

# Define mapping (schema)
mapping = {
    "mappings": {
        "properties": {
            "filename": {"type": "keyword"},
            "text": {"type": "text"}
        }
    }
}

# Create the index
if not es.indices.exists(index=INDEX_NAME):
    es.indices.create(index=INDEX_NAME, body=mapping)
    print(f"Index '{INDEX_NAME}' created.")
```

#### **3Ô∏è‚É£ Add PDFs to Elasticsearch**

```python
def index_pdf(filename, text):
    """Indexes a PDF document in Elasticsearch."""
    doc = {"filename": filename, "text": text}
    es.index(index=INDEX_NAME, body=doc)

# Example usage
index_pdf("example.pdf", "This is a sample PDF content.")
```

***

### üîç Step 3: Search PDFs in Elasticsearch

Now that PDFs are indexed, we can **search for keywords**.

#### **Example: Search for "machine learning" in PDFs**

```python
def search_pdfs(query):
    """Search PDFs using Elasticsearch."""
    search_query = {
        "query": {
            "match": {
                "text": query
            }
        }
    }
    results = es.search(index=INDEX_NAME, body=search_query)
    return results["hits"]["hits"]

# Example usage
results = search_pdfs("machine learning")
for r in results:
    print(f"Found in: {r['_source']['filename']}\nText: {r['_source']['text'][:200]}...\n")
```

‚úÖ **Elasticsearch now powers our keyword-based PDF search!**

***

# üß† Indexing PDFs in FAISS (Semantic Search)

Elasticsearch works well for **exact keyword matches**, but it **doesn‚Äôt understand meaning**.

To **search PDFs based on meaning**, we use **FAISS (Facebook AI Similarity Search)** with **text embeddings**.

***

### üì¶ Step 1: Install FAISS & Sentence-Transformers

```bash
pip install faiss-cpu sentence-transformers
```

***

### üöÄ Step 2: Generate Embeddings for PDFs

We‚Äôll use `sentence-transformers` to convert text into **numerical embeddings**.

#### **1Ô∏è‚É£ Load the Embedding Model**

```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer("all-MiniLM-L6-v2")  # Fast and accurate
```

#### **2Ô∏è‚É£ Convert PDF Text into Embeddings**

```python
def embed_text(text):
    """Generates an embedding for a given text."""
    return model.encode(text)

# Example usage
embedding = embed_text("This is a sample text.")
print(embedding.shape)  # Output: (384,)
```

***

### üî• Step 3: Store Embeddings in FAISS

Now, we create a FAISS index to store and search our embeddings.

#### **1Ô∏è‚É£ Import FAISS & Create an Index**

```python
import faiss
import numpy as np

DIMENSIONS = 384  # Model output size
index = faiss.IndexFlatL2(DIMENSIONS)  # L2 distance index
```

#### **2Ô∏è‚É£ Index PDF Embeddings**

```python
pdf_texts = {
    "example.pdf": "This document is about deep learning and AI.",
    "sample.pdf": "This paper discusses cloud computing concepts."
}

embeddings = np.array([embed_text(text) for text in pdf_texts.values()])
index.add(embeddings)

print("FAISS index created with", index.ntotal, "documents.")
```

***

### üîç Step 4: Search PDFs in FAISS

Now we can **search** using **semantic similarity**.

#### **1Ô∏è‚É£ Search FAISS Using a Query**

```python
def search_faiss(query, k=2):
    """Searches FAISS for the most similar PDFs."""
    query_embedding = embed_text(query).reshape(1, -1)
    D, I = index.search(query_embedding, k)  # Retrieve top-k
    return I

# Example usage
query = "AI and deep learning"
results = search_faiss(query)

for i in results[0]:
    print("Matched:", list(pdf_texts.keys())[i])
```

‚úÖ **FAISS now powers our semantic PDF search!**

***

<!-- 
## üéØ What We Built in Part 2

| Feature  | Status |
|----------|--------|
| **Keyword Search (Elasticsearch)** ‚úÖ Done |
| **Semantic Search (FAISS)** ‚úÖ Done |
| **Embeddings (sentence-transformers)** ‚úÖ Done |

---

## üîú Coming in Part 3: AI Chatbot with Llama 2  
Now that we can **retrieve relevant PDFs**, the next step is to **connect it to an LLM (Llama 2)** to build a chatbot!  

### **üîπ In Part 3, we‚Äôll cover:**  
‚úÖ Running **Llama 2** locally  
‚úÖ Using **retrieval-augmented generation (RAG)** to feed PDFs into the chatbot  
‚úÖ **Building a chat UI** with Streamlit  

Stay tuned for **Part 3!** üöÄ  

---

## üìö References  
- [Elasticsearch Docs](https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html)  
- [FAISS GitHub](https://github.com/facebookresearch/faiss)  
- [Sentence-Transformers](https://www.sbert.net/)  

--- -->

Here‚Äôs **Part 3**, where we integrate **Llama 2** to create an **AI chatbot that can answer questions based on our PDF data** using **retrieval-augmented generation (RAG)**. üöÄ

<!-- 
In **Part 1**, we extracted text from PDFs.  
In **Part 2**, we indexed that text in **Elasticsearch** (keyword search) and **FAISS** (semantic search).  

Now, we‚Äôll **use those search results to power an AI chatbot**, allowing us to **ask natural language questions** and get answers from our PDF collection.  

---

## üîç What We‚Äôll Cover in This Part:
‚úÖ Set up **Llama 2** locally (100% on-prem)  
‚úÖ Use **retrieval-augmented generation (RAG)** to feed search results into Llama 2  
‚úÖ Build a **simple chatbot UI** using Streamlit  

### üèóÔ∏è Recap of Our Tech Stack:
| Component             | Tool |
|-----------------------|---------------------|
| **Text Extraction**   | `PyMuPDF` (Done ‚úÖ) |
| **Keyword Search**    | `Elasticsearch` (Done ‚úÖ) |
| **Semantic Search**   | `FAISS` (Done ‚úÖ) |
| **Embedding Model**   | `sentence-transformers` (Done ‚úÖ) |
| **Chatbot LLM**       | `Llama 2` (Now) |
| **User Interface**    | `Streamlit` (Now) |

--- -->

# üî• Running Llama 2 Locally

### üèóÔ∏è Step 1: Install Llama 2

We‚Äôll use `llama-cpp-python`, which allows us to **run Llama 2 on CPU or GPU**.

```bash
pip install llama-cpp-python
```

üí° If you have a **powerful GPU**, use the `GGUF` version for better performance.

***

### üöÄ Step 2: Download a Llama 2 Model

Go to [Meta‚Äôs Llama 2 page](https://ai.meta.com/llama/) and download a model.

For **fast responses**, I recommend:

* `llama-2-7b-chat.Q4_K_M.gguf` (Quantized 4-bit model)
* `llama-2-13b-chat.Q4_K_M.gguf` (Larger but still manageable)

Place the model file in a folder called `models`.

***

### üî• Step 3: Load Llama 2 in Python

```python
from llama_cpp import Llama

# Load Llama 2 model
llm = Llama(model_path="models/llama-2-7b-chat.Q4_K_M.gguf")

# Example chat
response = llm("What is machine learning?")
print(response["choices"][0]["text"])
```

‚úÖ **Llama 2 is now running locally!**

***

# üß† Implementing RAG (Retrieval-Augmented Generation)

By itself, **Llama 2 doesn‚Äôt know about our PDFs**.

To **make it answer questions based on PDFs**, we use **retrieval-augmented generation (RAG)**:

1. **Search PDFs** using **Elasticsearch (keyword) and FAISS (semantic search)**
2. **Feed search results** into **Llama 2** as context
3. **Ask Llama 2 a question**, and it will generate an answer **based on the retrieved PDFs**

***

### üöÄ Step 1: Search PDFs Using Elasticsearch & FAISS

We **combine both search methods** to get the **most relevant** PDF chunks.

```python
def search_pdfs_rag(query, k=3):
    """Search PDFs using Elasticsearch (keyword) and FAISS (semantic)."""
    # 1Ô∏è‚É£ Keyword Search (Elasticsearch)
    es_results = search_pdfs(query)[:k]

    # 2Ô∏è‚É£ Semantic Search (FAISS)
    faiss_results = search_faiss(query, k)[:k]

    # 3Ô∏è‚É£ Merge and Return Results
    combined_results = set([r["_source"]["text"][:500] for r in es_results])
    combined_results.update([list(pdf_texts.values())[i][:500] for i in faiss_results[0]])

    return "\n\n".join(combined_results)
```

***

### üî• Step 2: Feed Search Results to Llama 2

Now we **pass the retrieved text as context** to Llama 2.

```python
def chat_with_pdfs(query):
    """Uses RAG to answer questions based on PDF content."""
    context = search_pdfs_rag(query)

    prompt = f"Use the following context to answer the question:\n\n{context}\n\nQuestion: {query}\nAnswer:"

    response = llm(prompt)
    return response["choices"][0]["text"]

# Example Usage
print(chat_with_pdfs("What is deep learning?"))
```

‚úÖ **Now, Llama 2 can answer questions based on our PDFs!**

***

# üé® Building a Simple Chat UI with Streamlit

To make this user-friendly, let‚Äôs build a **web-based chatbot** using **Streamlit**.

***

### üì¶ Step 1: Install Streamlit

```bash
pip install streamlit
```

***

### üöÄ Step 2: Create a Simple Chatbot UI

Create a file **`app.py`**:

```python
import streamlit as st

st.title("üìÑ AI Chatbot for PDF Search")

query = st.text_input("Ask a question:")
if query:
    response = chat_with_pdfs(query)
    st.write("### ü§ñ AI Response:")
    st.write(response)
```

***

### üéØ Step 3: Run the App

```bash
streamlit run app.py
```

‚úÖ **Now, you have a chatbot that searches PDFs and answers questions!**

***

<!-- 
## üéØ What We Built in Part 3

| Feature  | Status |
|----------|--------|
| **Llama 2 Running Locally** ‚úÖ Done |
| **RAG (PDF-Based Answers)** ‚úÖ Done |
| **Chatbot UI (Streamlit)** ‚úÖ Done |

---

---

## üìö References  
- [Llama 2](https://ai.meta.com/llama/)  
- [FAISS GitHub](https://github.com/facebookresearch/faiss)  
- [Elasticsearch Docs](https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html)  
- [Streamlit](https://streamlit.io/)  

---

‚úÖ **Next Up:** [On-Prem AI PDF Search - Part 4: Enhancing Search & UI](#) (Coming Soon)  
``` -->

<!-- Here‚Äôs **Part 4**, where we improve **search ranking, enhance the UI**, and add **PDF upload support** for a smoother experience. üöÄ   -->

<!-- 
Welcome to **Part 4** of this series, where we **improve search ranking, enhance the UI**, and **add PDF upload support** to make our chatbot more useful.  

In **Part 3**, we built a chatbot that:
‚úÖ Runs **Llama 2** locally  
‚úÖ Uses **retrieval-augmented generation (RAG)** to search PDFs  
‚úÖ Has a **basic chat UI with Streamlit**  

Now, we‚Äôll refine the **search results**, improve **usability**, and allow users to **upload new PDFs dynamically**.  

---

## üîç What We‚Äôll Cover in This Part:
‚úÖ Improve **search relevance & ranking**  
‚úÖ Enhance **Streamlit UI** (better layout & history)  
‚úÖ Add **PDF upload support** (update index dynamically)  

### üèóÔ∏è Recap of Our Tech Stack:
| Component             | Tool |
|-----------------------|---------------------|
| **Text Extraction**   | `PyMuPDF` (Done ‚úÖ) |
| **Keyword Search**    | `Elasticsearch` (Done ‚úÖ) |
| **Semantic Search**   | `FAISS` (Done ‚úÖ) |
| **Embedding Model**   | `sentence-transformers` (Done ‚úÖ) |
| **Chatbot LLM**       | `Llama 2` (Done ‚úÖ) |
| **User Interface**    | `Streamlit` (Enhancing Now) |

--- -->

# üîçImproving Search Ranking

Right now, our **Elasticsearch + FAISS** search returns **somewhat relevant** results, but we can improve **ranking & filtering**.

### üöÄ Step 1: Boost Keyword Matches in Elasticsearch

By default, Elasticsearch treats all matches equally. We can **boost results** that contain **exact keyword matches**.

#### ‚úÖ **Update Elasticsearch Search Query**

```python
def search_pdfs_improved(query, k=3):
    """Improves search ranking by boosting keyword matches."""
    search_query = {
        "query": {
            "bool": {
                "should": [
                    {"match": {"text": {"query": query, "boost": 2.0}}},  # Boost exact matches
                    {"match_phrase": {"text": {"query": query, "boost": 1.5}}}  # Boost phrase matches
                ]
            }
        }
    }
    results = es.search(index="pdf_documents", body=search_query)
    return results["hits"]["hits"][:k]

# Example usage
print(search_pdfs_improved("machine learning"))
```

‚úÖ **Boosts exact and phrase matches**\
‚úÖ **More relevant results** appear at the top

***

### üöÄ Step 2: Adjust FAISS to Prefer Recent Documents

FAISS **doesn‚Äôt consider document relevance**, but we can **re-rank results** based on recency.

#### ‚úÖ **Re-rank FAISS Results by Document Date**

```python
def rerank_faiss_results(faiss_results, doc_metadata):
    """Re-ranks FAISS results based on recency."""
    sorted_results = sorted(faiss_results, key=lambda doc: doc_metadata[doc]["date"], reverse=True)
    return sorted_results

# Example usage
metadata = {"example.pdf": {"date": "2024-01-01"}, "old.pdf": {"date": "2019-05-10"}}
print(rerank_faiss_results(["old.pdf", "example.pdf"], metadata))  # "example.pdf" comes first
```

‚úÖ **Recent documents now rank higher**

***

# üé® Enhancing Streamlit UI

Our **current chatbot UI is too basic**. Let‚Äôs:\
‚úÖ Improve layout\
‚úÖ Add chat history\
‚úÖ Show document sources

***

### üöÄ Step 1: Upgrade the Chat UI

Update **`app.py`** with a better layout:

```python
import streamlit as st

st.set_page_config(page_title="üìÑ AI Chatbot for PDFs", layout="wide")

st.title("üìÑ AI Chatbot for PDF Search")

# Sidebar
with st.sidebar:
    st.header("Settings")
    st.text("Customize your search")

query = st.text_input("Ask a question:")

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if query:
    response = chat_with_pdfs(query)
    st.session_state.chat_history.append((query, response))

st.write("### ü§ñ AI Response:")
for q, r in st.session_state.chat_history:
    st.write(f"**Q:** {q}")
    st.write(f"**A:** {r}")
    st.write("---")
```

‚úÖ **Keeps chat history**\
‚úÖ **Better layout with a sidebar**

***

### üöÄ Step 2: Show PDF Sources in Chat

Modify **`chat_with_pdfs()`** to **return sources**.

```python
def chat_with_pdfs(query):
    """Returns AI response + sources."""
    context, sources = search_pdfs_rag(query, return_sources=True)

    prompt = f"Use the following context to answer the question:\n\n{context}\n\nQuestion: {query}\nAnswer:"
    response = llm(prompt)

    return response["choices"][0]["text"], sources
```

Now, update **`app.py`** to **show sources**:

```python
response, sources = chat_with_pdfs(query)

st.write("### ü§ñ AI Response:")
st.write(response)

st.write("üìÇ **Sources:**")
for source in sources:
    st.write(f"- {source}")
```

‚úÖ **Users see which PDFs were used to generate answers**

***

# üìÇ Adding PDF Upload Support

Currently, we **preload PDFs**, but users **can‚Äôt upload new ones**. Let‚Äôs **fix that!**

***

### üöÄ Step 1: Add File Upload to Streamlit

Modify **`app.py`** to allow **users to upload PDFs**.

```python
uploaded_files = st.file_uploader("Upload PDFs", accept_multiple_files=True, type=["pdf"])

if uploaded_files:
    for uploaded_file in uploaded_files:
        bytes_data = uploaded_file.read()
        
        # Save file locally
        with open(f"pdf_documents/{uploaded_file.name}", "wb") as f:
            f.write(bytes_data)
        
        # Extract text and index it
        text = extract_text_from_pdf(f"pdf_documents/{uploaded_file.name}")
        index_pdf(uploaded_file.name, text)
        
    st.success("Files uploaded and indexed successfully!")
```

‚úÖ **Users can now upload PDFs, and they‚Äôre instantly indexed**

***

<!-- 
## üéØ What We Built in Part 4

| Feature  | Status |
|----------|--------|
| **Better Search Ranking (Boosted Keywords)** ‚úÖ Done |
| **Re-Ranked FAISS Results by Recency** ‚úÖ Done |
| **Enhanced Chat UI (History + Sidebar + Sources)** ‚úÖ Done |
| **PDF Upload Support** ‚úÖ Done |

---

## üîú Coming in Part 5: Optimizing Performance  
Now that our chatbot is **fully functional**, we‚Äôll focus on **optimizing performance**.

### **üîπ In Part 5, we‚Äôll cover:**  
‚úÖ Running Llama 2 **faster** (quantization & GPU acceleration)  
‚úÖ Improving **FAISS search speed**  
‚úÖ Scaling to **thousands of PDFs efficiently**   -->

<!-- ## üìö References  
- [Elasticsearch Docs](https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html)  
- [FAISS GitHub](https://github.com/facebookresearch/faiss)  
- [Streamlit](https://streamlit.io/)  

---
 -->

<!--  
‚úÖ **Next Up:** [On-Prem AI PDF Search - Part 5: Optimizing Performance](#) (Coming Soon)  
``` -->

Here‚Äôs **Part 5**, where we **optimize performance** by making **Llama 2 run faster**, improving **FAISS search speed**, and scaling to **thousands of PDFs efficiently**. üöÄ

***

<!-- # On-Prem AI Chatbot for PDF Search ‚Äì Part 5: Optimizing Performance

Welcome to **Part 5** of this series! Now that our **AI chatbot works**, it‚Äôs time to **make it faster and more efficient**.  

In **Part 4**, we improved search ranking, enhanced the UI, and added **PDF upload support**.  
Now, we‚Äôll focus on **speeding up everything** so we can handle **thousands of PDFs efficiently**.

---

## üöÄ What We‚Äôll Optimize in This Part:
‚úÖ **Speed up Llama 2** (quantization & GPU acceleration)  
‚úÖ **Optimize FAISS for large-scale search**  
‚úÖ **Improve Elasticsearch indexing & search speed**  

### üèóÔ∏è Recap of Our Tech Stack:
| Component             | Tool |
|-----------------------|---------------------|
| **Text Extraction**   | `PyMuPDF` (Done ‚úÖ) |
| **Keyword Search**    | `Elasticsearch` (Done ‚úÖ) |
| **Semantic Search**   | `FAISS` (Done ‚úÖ) |
| **Embedding Model**   | `sentence-transformers` (Done ‚úÖ) |
| **Chatbot LLM**       | `Llama 2` (Optimizing Now) |
| **User Interface**    | `Streamlit` (Optimized in Part 4) |

--- -->

# ‚ö°Speeding Up Llama 2

By default, **Llama 2 can be slow**, especially on CPUs. Here‚Äôs how to **run it faster**.

***

### üöÄ Step 1: Use a **Quantized Llama 2 Model**

Quantization **reduces model size** and **speeds up inference**.

#### ‚úÖ **Download a Quantized GGUF Model**

Go to [Meta‚Äôs Llama 2 page](https://ai.meta.com/llama/) and download:

* `llama-2-7b-chat.Q4_K_M.gguf` (4-bit quantized)
* **OR** `llama-2-13b-chat.Q4_K_M.gguf` (faster than full precision)

Move it to `models/`.

***

### üöÄ Step 2: Enable GPU Acceleration (If Available)

If you have a **GPU**, use `llama-cpp-python` with CUDA.

#### ‚úÖ **Install CUDA & llama-cpp-python with GPU Support**

```bash
CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python --no-cache-dir
```

Then, modify your **Llama 2 loading code**:

```python
from llama_cpp import Llama

# Load Llama 2 model with GPU acceleration
llm = Llama(model_path="models/llama-2-7b-chat.Q4_K_M.gguf", n_gpu_layers=100)
```

‚úÖ **Massive speed boost on GPUs!**\
‚úÖ **Even CPU inference is faster with quantization**

***

### üöÄ Step 3: Reduce Response Time with Streaming

Right now, **Llama 2 waits for the full response** before returning anything.

We can **stream responses** as they‚Äôre generated for a **faster, chat-like feel**.

#### ‚úÖ **Modify `chat_with_pdfs()` to Stream Responses**

```python
def chat_with_pdfs(query):
    """Streams responses from Llama 2 for faster user experience."""
    context = search_pdfs_rag(query)
    prompt = f"Use the following context to answer:\n\n{context}\n\nQuestion: {query}\nAnswer:"

    for response in llm(prompt, stream=True):
        yield response["choices"][0]["text"]
```

‚úÖ **Now responses appear instantly instead of waiting!**

***

# üèéÔ∏è Optimizing FAISS for Large-Scale Search

FAISS is **fast**, but it can slow down as we **add more PDFs**.

Here‚Äôs how to **speed it up for thousands of documents**.

***

### üöÄ Step 1: Use **HNSW Indexing** Instead of Flat L2

By default, FAISS uses **brute-force search** (`IndexFlatL2`).\
For **huge datasets**, we should use **Hierarchical Navigable Small World (HNSW)** indexing.

#### ‚úÖ **Modify FAISS Index to Use HNSW**

```python
import faiss

DIMENSIONS = 384  # Sentence-Transformer output size
index = faiss.IndexHNSWFlat(DIMENSIONS, 32)  # 32 is the max number of links per node
```

‚úÖ **Now FAISS search is MUCH faster** for large datasets

***

### üöÄ Step 2: Use **IVF Indexing for Faster Lookups**

Another trick is **Inverted File Index (IVF)**, which clusters vectors for fast retrieval.

#### ‚úÖ **Modify FAISS Index to Use IVF**

```python
num_clusters = 128  # Adjust based on dataset size
quantizer = faiss.IndexFlatL2(DIMENSIONS)  
index = faiss.IndexIVFFlat(quantizer, DIMENSIONS, num_clusters)
index.train(embeddings)  # Train on initial dataset
```

‚úÖ **Speeds up searches by grouping similar documents**

***

# üöÄ Scaling Elasticsearch for Massive PDF Collections

If you have **millions of PDFs**, **Elasticsearch needs tuning**.

***

### üöÄ Step 1: Disable Refresh for Bulk Indexing

By default, Elasticsearch **refreshes after every document insert**, slowing down indexing.

#### ‚úÖ **Disable Refresh While Indexing**

```python
es.indices.put_settings(index="pdf_documents", body={"refresh_interval": "-1"})

# Bulk index PDFs
for filename, text in pdf_texts.items():
    index_pdf(filename, text)

# Re-enable refresh
es.indices.put_settings(index="pdf_documents", body={"refresh_interval": "1s"})
```

‚úÖ **Indexing PDFs is now 5-10x faster**

***

### üöÄ Step 2: Increase Shard Count for Large Datasets

For **huge** collections, increase the number of **shards**.

#### ‚úÖ **Modify Index Settings**

```python
index_settings = {
    "settings": {
        "index": {
            "number_of_shards": 3,
            "number_of_replicas": 1
        }
    }
}
es.indices.create(index="pdf_documents", body=index_settings)
```

‚úÖ **Speeds up searches & indexing on large datasets**

***

<!-- 
# üéØ What We Optimized in Part 5  

| Optimization  | Status |
|--------------|--------|
| **Llama 2 Quantization (4-bit GGUF)** ‚úÖ Done |
| **Enable GPU Acceleration** ‚úÖ Done |
| **Stream Responses for Faster Chat** ‚úÖ Done |
| **Switch FAISS to HNSW Indexing** ‚úÖ Done |
| **Use FAISS IVF for Faster Lookups** ‚úÖ Done |
| **Disable Elasticsearch Refresh for Bulk Indexing** ‚úÖ Done |
| **Increase Elasticsearch Shard Count for Scale** ‚úÖ Done |

---

## üîú Coming in Part 6: Advanced AI Enhancements  

Our chatbot **now runs fast**, but we can **make it even smarter**.  

### **üîπ In Part 6, we‚Äôll cover:**  
‚úÖ Using **multi-turn memory** (remembering past questions)  
‚úÖ Improving **response accuracy** with **fine-tuned Llama 2**  
‚úÖ Enhancing **UI with context-aware responses**  

Stay tuned for **Part 6!** üöÄ  

---

## üìö References  
- [Llama 2](https://ai.meta.com/llama/)  
- [FAISS GitHub](https://github.com/facebookresearch/faiss)  
- [Elasticsearch Docs](https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html)  

---

‚úÖ **Next Up:** [On-Prem AI PDF Search - Part 6: Advanced AI Enhancements](#) (Coming Soon)  
``` -->

<!-- 
Here's **Part 6**, where we introduce **multi-turn memory**, improve **response accuracy** with **fine-tuned Llama 2**, and enhance the **chat UI with context-aware responses**. üöÄ  

---


# On-Prem AI Chatbot for PDF Search ‚Äì Part 6: Advanced AI Enhancements  

Welcome to **Part 6** of this series! Now that our chatbot **runs fast**, let‚Äôs make it **smarter**.  

In **Part 5**, we optimized performance by:
‚úÖ Speeding up **Llama 2**  
‚úÖ Improving **FAISS search efficiency**  
‚úÖ Scaling **Elasticsearch for large datasets**  

Now, we‚Äôll focus on **AI enhancements**:  
‚úÖ Adding **multi-turn memory** (so the chatbot remembers context)  
‚úÖ Fine-tuning **Llama 2** for **better responses**  
‚úÖ Improving **UI for a better user experience**  

---

## üèóÔ∏è What We‚Äôll Improve in This Part  

| Feature  | Enhancement |
|----------|------------|
| **Memory** | Multi-turn conversation memory |
| **LLM Accuracy** | Fine-tuning Llama 2 on domain-specific PDFs |
| **Chat UI** | Better response formatting & context awareness |

### **Recap of Our Tech Stack**
| Component             | Tool |
|-----------------------|---------------------|
| **Text Extraction**   | `PyMuPDF` (Done ‚úÖ) |
| **Keyword Search**    | `Elasticsearch` (Done ‚úÖ) |
| **Semantic Search**   | `FAISS` (Done ‚úÖ) |
| **Embedding Model**   | `sentence-transformers` (Done ‚úÖ) |
| **Chatbot LLM**       | `Llama 2` (Enhancing Now) |
| **User Interface**    | `Streamlit` (Improving Now) | -->

***

# üß†Adding Multi-Turn Memory

By default, **Llama 2 only answers one question at a time**.

To enable **multi-turn conversation memory**, we need to **track past questions and answers**.

***

### üöÄ Step 1: Modify `chat_with_pdfs()` to Include Memory

Modify the chatbot function to **store past questions & responses**.

```python
def chat_with_pdfs(query):
    """Uses multi-turn memory for AI conversations."""
    
    # Retrieve relevant PDFs
    context, sources = search_pdfs_rag(query, return_sources=True)
    
    # Maintain conversation memory
    if "conversation_history" not in st.session_state:
        st.session_state.conversation_history = []
    
    # Create conversation context
    conversation_history = "\n".join(st.session_state.conversation_history)
    
    # Generate response using Llama 2
    prompt = f"""
    Previous conversation:
    {conversation_history}
    
    Use the following PDF context to answer the question:
    {context}
    
    Question: {query}
    Answer:
    """
    
    response = llm(prompt)["choices"][0]["text"]
    
    # Save conversation
    st.session_state.conversation_history.append(f"Q: {query}\nA: {response}")
    
    return response, sources
```

‚úÖ **Now, the chatbot remembers previous questions!**

***

### üöÄ Step 2: Display Conversation History in the UI

Modify **`app.py`** to **show chat history**.

```python
import streamlit as st

st.title("üìÑ AI Chatbot for PDF Search")

query = st.text_input("Ask a question:")

if query:
    response, sources = chat_with_pdfs(query)

    st.write("### ü§ñ AI Response:")
    st.write(response)

    # Display conversation history
    st.write("### üìù Conversation History:")
    for message in st.session_state.conversation_history[-5:]:  # Show last 5 messages
        st.write(message)

    # Show document sources
    st.write("üìÇ **Sources:**")
    for source in sources:
        st.write(f"- {source}")
```

‚úÖ **Now users see chat history & sources in a clean format**

***

# üèãÔ∏è‚Äç‚ôÇÔ∏è Fine-Tuning Llama 2 for Better Responses

Currently, **Llama 2 isn‚Äôt optimized for our PDFs**.\
Fine-tuning **makes it much smarter** about our documents.

***

### üöÄ Step 1: Prepare Custom Training Data

Fine-tuning requires **examples of questions & correct answers**.\
We‚Äôll use **our own PDFs** to create a dataset.

#### ‚úÖ **Format Training Data in JSON**

```json
[
    {
        "input": "What is machine learning?",
        "output": "Machine learning is a method of data analysis that automates analytical model building."
    },
    {
        "input": "Explain deep learning.",
        "output": "Deep learning is a subset of machine learning that uses neural networks to model complex patterns in data."
    }
]
```

Save this as **`training_data.json`**.

***

### üöÄ Step 2: Fine-Tune Llama 2

We‚Äôll use **Hugging Face‚Äôs `transformers`** to fine-tune Llama 2.

#### ‚úÖ **Install Dependencies**

```bash
pip install transformers datasets peft
```

#### ‚úÖ **Fine-Tune Llama 2**

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
import torch
import json

# Load base model & tokenizer
model_name = "meta-llama/Llama-2-7b-chat-hf"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Load training data
with open("training_data.json", "r") as f:
    training_data = json.load(f)

# Convert to tokenized format
train_texts = [d["input"] for d in training_data]
train_labels = [d["output"] for d in training_data]

train_encodings = tokenizer(train_texts, padding=True, truncation=True, return_tensors="pt")
label_encodings = tokenizer(train_labels, padding=True, truncation=True, return_tensors="pt")

# Fine-tuning settings
training_args = TrainingArguments(
    output_dir="./fine-tuned-llama",
    per_device_train_batch_size=2,
    num_train_epochs=3,
    save_strategy="epoch"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_encodings,
    eval_dataset=label_encodings
)

trainer.train()
model.save_pretrained("./fine-tuned-llama")
tokenizer.save_pretrained("./fine-tuned-llama")
```

‚úÖ **Llama 2 is now fine-tuned on our PDFs!**

***

<!-- 
# üéØ What We Improved in Part 6  

| Feature  | Status |
|----------|--------|
| **Multi-Turn Memory (Chat History)** ‚úÖ Done |
| **Better UI with Chat Context** ‚úÖ Done |
| **Fine-Tuned Llama 2 for Better Answers** ‚úÖ Done |

---

## üîú Coming in Part 7: Deploying the AI Chatbot  

Now that our chatbot **remembers conversations & gives better answers**,  
let‚Äôs **deploy it in production** with **GPU support & a web server**.

### **üîπ In Part 7, we‚Äôll cover:**  
‚úÖ Deploying with **FastAPI & Gunicorn**  
‚úÖ Running **Llama 2 on a dedicated AI server**  
‚úÖ Scaling for **multiple users**   -->

<!-- 
## üìö References  
- [Llama 2](https://ai.meta.com/llama/)  
- [FAISS GitHub](https://github.com/facebookresearch/faiss)  
- [Hugging Face Fine-Tuning Guide](https://huggingface.co/docs/transformers/training)  

---

‚úÖ **Next Up:** [On-Prem AI PDF Search - Part 7: Deploying the AI Chatbot](#) (Coming Soon)  
```
 -->

<!-- 
Here's **Part 7**, where we **deploy the AI chatbot** using **FastAPI, Gunicorn, and Streamlit**, and optimize it for **multiple users & production environments**. üöÄ   -->

<!-- # On-Prem AI Chatbot for PDF Search ‚Äì Part 7: Deploying the AI Chatbot  

Welcome to **Part 7** of this series! Now that our **chatbot remembers conversations & provides accurate answers**, it‚Äôs time to **deploy it for real-world use**.  

In **Part 6**, we:  
‚úÖ Added **multi-turn memory** for conversations  
‚úÖ Fine-tuned **Llama 2 for better responses**  
‚úÖ Improved **UI with chat history & sources**  

Now, we‚Äôll **deploy the chatbot** so it can be accessed by multiple users!  

---

## üöÄ What We‚Äôll Cover in This Part:
‚úÖ Deploy with **FastAPI** (for API-based access)  
‚úÖ Use **Gunicorn & Uvicorn** for performance  
‚úÖ Run **Streamlit on a web server**  
‚úÖ Optimize **Llama 2 for multi-user concurrency**  

### **Recap of Our Tech Stack**
| Component             | Tool |
|-----------------------|---------------------|
| **Text Extraction**   | `PyMuPDF` (Done ‚úÖ) |
| **Keyword Search**    | `Elasticsearch` (Done ‚úÖ) |
| **Semantic Search**   | `FAISS` (Done ‚úÖ) |
| **Embedding Model**   | `sentence-transformers` (Done ‚úÖ) |
| **Chatbot LLM**       | `Llama 2` (Deploying Now) |
| **User Interface**    | `Streamlit` (Deploying Now) |

---
 -->

# ‚ö°  Deploying the Chatbot Backend with FastAPI

FastAPI is a **lightweight, high-performance API framework** that will serve as our chatbot‚Äôs **backend**.

***

### üöÄ Step 1: Install FastAPI & Uvicorn

```bash
pip install fastapi uvicorn gunicorn
```

***

### üöÄ Step 2: Create the FastAPI Server

Create a new file **`server.py`**:

```python
from fastapi import FastAPI
from pydantic import BaseModel
from llama_cpp import Llama

app = FastAPI()

# Load Llama 2 model
llm = Llama(model_path="models/llama-2-7b-chat.Q4_K_M.gguf", n_gpu_layers=100)

# Define request schema
class ChatRequest(BaseModel):
    query: str

@app.post("/chat")
def chat(request: ChatRequest):
    """Handles chat requests and returns AI responses."""
    response = llm(request.query)["choices"][0]["text"]
    return {"response": response}
```

***

### üöÄ Step 3: Run FastAPI Server

Start the server using Uvicorn:

```bash
uvicorn server:app --host 0.0.0.0 --port 8000
```

‚úÖ **Now, our chatbot runs as an API!**

Test it with:

```bash
curl -X POST "http://localhost:8000/chat" -H "Content-Type: application/json" -d '{"query": "What is machine learning?"}'
```

***

# üèéÔ∏è Scaling with Gunicorn

Uvicorn runs a **single process**, which isn‚Äôt ideal for **multiple users**.\
We use **Gunicorn** to **run multiple workers**.

### üöÄ Step 1: Run FastAPI with Gunicorn

```bash
gunicorn -w 4 -k uvicorn.workers.UvicornWorker server:app --bind 0.0.0.0:8000
```

‚úÖ **Now, our chatbot can handle multiple users at once!**

***

# üé® Deploying Streamlit as a Frontend

Now that the API is live, we‚Äôll **connect it to Streamlit**.

***

### üöÄ Step 1: Modify `app.py` to Call FastAPI

Update **`app.py`** to fetch chatbot responses via API.

```python
import streamlit as st
import requests

st.title("üìÑ AI Chatbot for PDF Search")

query = st.text_input("Ask a question:")

if query:
    response = requests.post("http://localhost:8000/chat", json={"query": query}).json()
    st.write("### ü§ñ AI Response:")
    st.write(response["response"])
```

***

### üöÄ Step 2: Run Streamlit on a Web Server

Run Streamlit with:

```bash
streamlit run app.py --server.port 8501 --server.address 0.0.0.0
```

‚úÖ **Now, the chatbot has a web interface!**

***

# üèóÔ∏è  Running Everything with Supervisor

To keep **FastAPI & Streamlit running in the background**, use **Supervisor**.

### üöÄ Step 1: Install Supervisor

```bash
sudo apt install supervisor
```

***

### üöÄ Step 2: Create Supervisor Config

Create **`/etc/supervisor/conf.d/chatbot.conf`**:

```ini
[program:fastapi_server]
command=/usr/bin/gunicorn -w 4 -k uvicorn.workers.UvicornWorker server:app --bind 0.0.0.0:8000
autostart=true
autorestart=true
stderr_logfile=/var/log/fastapi_server.err.log
stdout_logfile=/var/log/fastapi_server.out.log

[program:streamlit_ui]
command=/usr/bin/streamlit run /path/to/app.py --server.port 8501 --server.address 0.0.0.0
autostart=true
autorestart=true
stderr_logfile=/var/log/streamlit_ui.err.log
stdout_logfile=/var/log/streamlit_ui.out.log
```

Reload Supervisor:

```bash
sudo supervisorctl reread
sudo supervisorctl update
sudo supervisorctl start fastapi_server
sudo supervisorctl start streamlit_ui
```

‚úÖ **Now, FastAPI & Streamlit start automatically on reboot!**

***

<!-- 
# üéØ What We Deployed in Part 7  

| Feature  | Status |
|----------|--------|
| **FastAPI Backend (Chat API)** ‚úÖ Done |
| **Gunicorn for Multi-User Support** ‚úÖ Done |
| **Streamlit UI Connected to API** ‚úÖ Done |
| **Supervisor for Auto-Restart** ‚úÖ Done |

---

## üîú Coming in Part 8: Enhancing Security & User Management  

Now that our chatbot **runs in production**, we‚Äôll add **security features**.

### **üîπ In Part 8, we‚Äôll cover:**  
‚úÖ Adding **authentication (API keys & login)**  
‚úÖ **Rate-limiting** to prevent abuse  
‚úÖ Encrypting **user queries & responses**   -->

<!-- 
## üìö References  
- [FastAPI Docs](https://fastapi.tiangolo.com/)  
- [Gunicorn Docs](https://gunicorn.org/)  
- [Supervisor Docs](http://supervisord.org/)  
- [Streamlit](https://streamlit.io/)  

---

‚úÖ **Next Up:** [On-Prem AI PDF Search - Part 8: Security & User Management](#) (Coming Soon)  
```

---
 -->

<!-- 

Here‚Äôs **Part 8**, where we add **security and user management** by implementing **API authentication, rate-limiting, and encryption** to protect our on-prem AI chatbot. üöÄ   -->

***

<!-- 

# On-Prem AI Chatbot for PDF Search ‚Äì Part 8: Security & User Management  

Welcome to **Part 8** of this series! Now that our chatbot is **deployed**, we need to **secure it against unauthorized access and abuse**.  

In **Part 7**, we:  
‚úÖ Deployed **FastAPI** as a backend  
‚úÖ Used **Gunicorn** for **multi-user support**  
‚úÖ Connected **Streamlit** as the front-end  

Now, we‚Äôll **secure the chatbot** with:  
‚úÖ **Authentication** (API keys & user login)  
‚úÖ **Rate-limiting** (to prevent abuse)  
‚úÖ **Encryption** (for safe data transmission)  

---

## üîê What We‚Äôll Secure in This Part  

| Security Feature | Implementation |
|-----------------|---------------|
| **Authentication** | API Key & OAuth2 Login |
| **Rate-Limiting** | Prevent spam/bot abuse |
| **Encryption** | Encrypt user queries |

### **Recap of Our Tech Stack**
| Component             | Tool |
|-----------------------|---------------------|
| **FastAPI Backend**   | Securing Now üîê |
| **Gunicorn Server**   | Securing Now üîê |
| **Streamlit UI**      | Securing Now üîê |

--- -->

# üîë API Authentication with API Keys

Right now, **anyone can access our chatbot API**. We‚Äôll restrict access using **API keys**.

***

### üöÄ Step 1: Generate API Keys for Users

Modify **`server.py`** to store API keys.

```python
API_KEYS = {
    "user1": "abc123",
    "admin": "xyz789"
}

def verify_api_key(api_key: str):
    """Checks if the provided API key is valid."""
    return api_key in API_KEYS.values()
```

***

### üöÄ Step 2: Require API Key for Chat Requests

Modify the `/chat` route to **require an API key**.

```python
from fastapi import FastAPI, Header, HTTPException

app = FastAPI()

@app.post("/chat")
def chat(query: str, api_key: str = Header(None)):
    """Requires API key for chatbot access."""
    
    if not api_key or not verify_api_key(api_key):
        raise HTTPException(status_code=401, detail="Invalid API Key")

    response = llm(query)["choices"][0]["text"]
    return {"response": response}
```

‚úÖ **Now, only users with a valid API key can access the chatbot!**

Test it with:

```bash
curl -X POST "http://localhost:8000/chat" -H "API-Key: abc123" -H "Content-Type: application/json" -d '{"query": "What is AI?"}'
```

***

# ‚è≥ Preventing API Abuse with Rate-Limiting

To prevent **spam/bot abuse**, we‚Äôll **limit how often users can query the API**.

***

### üöÄ Step 1: Install Rate-Limiting Middleware

Install **`slowapi`**, a FastAPI-compatible rate limiter.

```bash
pip install slowapi
```

***

### üöÄ Step 2: Add Rate-Limiting to FastAPI

Modify **`server.py`**:

```python
from slowapi import Limiter
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)

@app.post("/chat")
@limiter.limit("5/minute")
def chat(query: str, api_key: str = Header(None)):
    """Limits requests to 5 per minute per user."""
    
    if not api_key or not verify_api_key(api_key):
        raise HTTPException(status_code=401, detail="Invalid API Key")

    response = llm(query)["choices"][0]["text"]
    return {"response": response}
```

‚úÖ **Now, users can only make 5 requests per minute!**

Test it by **sending multiple requests** in a short time.

***

# üîí Part 3: Encrypting User Queries

By default, **data is sent in plaintext**. Let‚Äôs **encrypt user queries** to **protect sensitive data**.

***

### üöÄ Step 1: Install Cryptography Library

```bash
pip install cryptography
```

***

### üöÄ Step 2: Encrypt User Queries Before Sending

Modify **Streamlit UI (`app.py`)**:

```python
from cryptography.fernet import Fernet
import requests

# Generate encryption key (Only run once!)
key = Fernet.generate_key()
cipher = Fernet(key)

st.title("üîê Secure AI Chatbot")

query = st.text_input("Ask a question:")

if query:
    encrypted_query = cipher.encrypt(query.encode()).decode()
    
    response = requests.post("http://localhost:8000/chat", json={"query": encrypted_query})
    
    decrypted_response = cipher.decrypt(response.json()["response"].encode()).decode()
    
    st.write("### ü§ñ AI Response:")
    st.write(decrypted_response)
```

‚úÖ **Now, queries & responses are encrypted before being sent!**

***

# üõ†Ô∏è Securing Deployment with HTTPS

To enable **secure communication**, use **Let‚Äôs Encrypt** for **SSL/TLS encryption**.

***

### üöÄ Step 1: Install Certbot

```bash
sudo apt install certbot python3-certbot-nginx
```

***

### üöÄ Step 2: Configure SSL for Nginx

Modify **`/etc/nginx/sites-available/chatbot`**:

```nginx
server {
    listen 80;
    server_name yourdomain.com;
    return 301 https://$host$request_uri;
}

server {
    listen 443 ssl;
    server_name yourdomain.com;

    ssl_certificate /etc/letsencrypt/live/yourdomain.com/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/yourdomain.com/privkey.pem;

    location / {
        proxy_pass http://localhost:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
```

Restart Nginx:

```bash
sudo systemctl restart nginx
```

‚úÖ **Now, the chatbot runs securely over HTTPS!**

<!-- ---

# üéØ What We Secured in Part 8  

| Security Feature  | Status |
|------------------|--------|
| **API Authentication (API Keys)** ‚úÖ Done |
| **Rate-Limiting (Prevent Spam/Bots)** ‚úÖ Done |
| **Query Encryption** ‚úÖ Done |
| **HTTPS for Secure Communication** ‚úÖ Done |

---

## üîú Coming in Part 9: Logging & Analytics  

Now that our chatbot **is secure**, we‚Äôll add **logging & analytics** to track usage.

### **üîπ In Part 9, we‚Äôll cover:**  
‚úÖ Logging **all chatbot interactions**  
‚úÖ Tracking **most common questions**  
‚úÖ Generating **analytics reports**  

Stay tuned for **Part 9!** üöÄ  

---

## üìö References  
- [FastAPI Docs](https://fastapi.tiangolo.com/)  
- [SlowAPI Rate-Limiting](https://github.com/laurentS/slowapi)  
- [Cryptography Docs](https://cryptography.io/)  
- [Let‚Äôs Encrypt](https://letsencrypt.org/)  

---

‚úÖ **Next Up:** [On-Prem AI PDF Search - Part 9: Logging & Analytics](#) (Coming Soon)  
```

--- -->

<!-- 
Here‚Äôs **Part 9**, where we **add logging and analytics** to track chatbot usage, log interactions, and generate insights. üöÄ  

---



# On-Prem AI Chatbot for PDF Search ‚Äì Part 9: Logging & Analytics  

Welcome to **Part 9** of this series! Now that our chatbot **is deployed and secure**, it‚Äôs time to **track its performance**.  

In **Part 8**, we:  
‚úÖ Secured the chatbot with **API authentication**  
‚úÖ Added **rate-limiting** to prevent spam  
‚úÖ Encrypted **queries & responses** for security  

Now, we‚Äôll **log all chatbot interactions** and generate **analytics reports** to understand:  
‚úÖ **Who is using the chatbot?**  
‚úÖ **What are the most common questions?**  
‚úÖ **How is the chatbot performing over time?**  

---

## üìä What We‚Äôll Track in This Part  

| Feature  | Implementation |
|----------|---------------|
| **Chatbot Logging** | Log user queries & responses |
| **Usage Tracking** | Track most common queries |
| **Performance Analytics** | Monitor chatbot response times |

### **Recap of Our Tech Stack**
| Component             | Tool |
|-----------------------|---------------------|
| **FastAPI Backend**   | Logging Now üìù |
| **Gunicorn Server**   | Logging Now üìù |
| **Streamlit UI**      | Displaying Analytics üìä |

--- -->

# üìú Logging User Interactions

We‚Äôll **log every chatbot request** to a database for later analysis.

***

### üöÄ Step 1: Install SQLite for Logging

We‚Äôll store logs in an **SQLite database**.

```bash
pip install sqlite3
```

***

### üöÄ Step 2: Modify FastAPI to Log Chats

Modify **`server.py`**:

```python
import sqlite3
from datetime import datetime

# Connect to SQLite database
conn = sqlite3.connect("chat_logs.db")
cursor = conn.cursor()

# Create logs table
cursor.execute("""
CREATE TABLE IF NOT EXISTS chat_logs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    timestamp TEXT,
    user TEXT,
    query TEXT,
    response TEXT
)
""")
conn.commit()

def log_chat(user, query, response):
    """Logs chatbot interactions to the database."""
    timestamp = datetime.now().isoformat()
    cursor.execute("INSERT INTO chat_logs (timestamp, user, query, response) VALUES (?, ?, ?, ?)",
                   (timestamp, user, query, response))
    conn.commit()

@app.post("/chat")
def chat(query: str, api_key: str = Header(None)):
    """Handles chatbot requests and logs them."""
    
    if not api_key or not verify_api_key(api_key):
        raise HTTPException(status_code=401, detail="Invalid API Key")

    response = llm(query)["choices"][0]["text"]
    
    # Log interaction
    log_chat(api_key, query, response)
    
    return {"response": response}
```

‚úÖ **Now, all chatbot interactions are logged!**

***

# üìà Tracking Most Common Queries

Now that **chats are logged**, let‚Äôs track **the most frequently asked questions**.

***

### üöÄ Step 1: Query Most Common Searches

Modify **`server.py`** to fetch analytics:

```python
@app.get("/analytics/top-queries")
def top_queries():
    """Returns the top 5 most asked questions."""
    cursor.execute("SELECT query, COUNT(query) as count FROM chat_logs GROUP BY query ORDER BY count DESC LIMIT 5")
    results = cursor.fetchall()
    return {"top_queries": results}
```

‚úÖ **Now we can see the top 5 queries!**

Test it:

```bash
curl -X GET "http://localhost:8000/analytics/top-queries"
```

***

# ‚è≥ Monitoring Response Time

To track **how fast the chatbot is responding**, we‚Äôll log **execution time**.

***

### üöÄ Step 1: Modify FastAPI to Track Response Time

Modify **`server.py`**:

```python
import time

@app.post("/chat")
def chat(query: str, api_key: str = Header(None)):
    """Logs chatbot response times for performance tracking."""
    
    if not api_key or not verify_api_key(api_key):
        raise HTTPException(status_code=401, detail="Invalid API Key")

    start_time = time.time()
    response = llm(query)["choices"][0]["text"]
    end_time = time.time()
    
    response_time = round(end_time - start_time, 2)
    
    # Log response time
    cursor.execute("INSERT INTO chat_logs (timestamp, user, query, response) VALUES (?, ?, ?, ?)",
                   (datetime.now().isoformat(), api_key, query, f"{response} (Response Time: {response_time}s)"))
    conn.commit()
    
    return {"response": response, "response_time": response_time}
```

‚úÖ **Now, chatbot response times are tracked!**

Test it:

```bash
curl -X POST "http://localhost:8000/chat" -H "API-Key: abc123" -H "Content-Type: application/json" -d '{"query": "How does machine learning work?"}'
```

***

# üìä Displaying Analytics in Streamlit

We‚Äôll display **usage insights** in a **dashboard**.

***

### üöÄ Step 1: Modify Streamlit UI

Update **`app.py`**:

```python
import streamlit as st
import requests

st.title("üìä Chatbot Analytics")

# Fetch top queries
response = requests.get("http://localhost:8000/analytics/top-queries").json()
top_queries = response["top_queries"]

# Display analytics
st.write("### üî• Top 5 Most Asked Questions")
for query, count in top_queries:
    st.write(f"- {query} ({count} times)")

# Fetch recent chats
st.write("### üìù Recent Chat Logs")
conn = sqlite3.connect("chat_logs.db")
cursor = conn.cursor()
cursor.execute("SELECT timestamp, user, query FROM chat_logs ORDER BY timestamp DESC LIMIT 5")
recent_chats = cursor.fetchall()

for timestamp, user, query in recent_chats:
    st.write(f"üìÖ {timestamp} - **{user}** asked: *{query}*")
```

‚úÖ **Now, we have a real-time analytics dashboard!**

Run it:

```bash
streamlit run app.py --server.port 8502
```

<!-- 
---

# üéØ What We Added in Part 9  

| Feature  | Status |
|----------|--------|
| **Logging User Queries & Responses** ‚úÖ Done |
| **Tracking Most Common Questions** ‚úÖ Done |
| **Monitoring Response Time** ‚úÖ Done |
| **Building an Analytics Dashboard** ‚úÖ Done |

---

## üîú Coming in Part 10: Automating Updates & Maintenance  

Now that we have **logging & analytics**, we‚Äôll automate **data updates**.

### **üîπ In Part 10, we‚Äôll cover:**  
‚úÖ Automatically **deleting old logs**  
‚úÖ Setting up **cron jobs** for maintenance  
‚úÖ Sending **alerts if chatbot response time is slow**  

Stay tuned for **Part 10!** üöÄ  

---

## üìö References  
- [FastAPI Docs](https://fastapi.tiangolo.com/)  
- [SQLite Docs](https://www.sqlite.org/)  
- [Streamlit Docs](https://streamlit.io/)  

--- -->

<!-- 
======================================
========================================

========================

Here‚Äôs **Part 10**, where we **automate updates and maintenance** for our AI chatbot by setting up **log rotation, database cleanup, and monitoring alerts**. üöÄ  


# On-Prem AI Chatbot for PDF Search ‚Äì Part 10: Automating Updates & Maintenance  

Welcome to **Part 10** of this series! Now that our chatbot **logs interactions and tracks analytics**, we need to **automate updates and maintenance**.  

In **Part 9**, we:  
‚úÖ Logged **chatbot interactions** into a database  
‚úÖ Tracked **most common user queries**  
‚úÖ Built a **real-time analytics dashboard**  

Now, we‚Äôll:  
‚úÖ **Rotate logs** to prevent database bloat  
‚úÖ **Automate database cleanup** to remove old data  
‚úÖ **Monitor response times** and **send alerts** if the chatbot is slow  

---

## üîÑ What We‚Äôll Automate in This Part  

| Task | Solution |
|------|----------|
| **Log Rotation** | Keep logs small by deleting old entries |
| **Database Cleanup** | Auto-delete chats older than 30 days |
| **System Monitoring** | Send alerts if chatbot slows down |

### **Recap of Our Tech Stack**
| Component             | Tool |
|-----------------------|---------------------|
| **FastAPI Backend**   | Automating Now üîÑ |
| **SQLite Database**   | Cleaning Now üßπ |
| **Streamlit UI**      | Monitoring Now üìä |

---

# üóÇÔ∏è Part 1: Automating Log Rotation  

Right now, **chat logs grow indefinitely**. We need to **automatically delete old logs**.

---

### üöÄ Step 1: Create a Cleanup Function  

Modify **`server.py`**:

```python
def cleanup_logs():
    """Deletes logs older than 30 days to prevent database bloat."""
    cursor.execute("DELETE FROM chat_logs WHERE timestamp < datetime('now', '-30 days')")
    conn.commit()
    print("üßπ Old logs deleted.")
```

‚úÖ **Now, old logs won‚Äôt clog the system!**  

---

### üöÄ Step 2: Run Cleanup Automatically  

We‚Äôll use a **cron job** to run this **every day**.

1Ô∏è‚É£ Open the cron editor:
```bash
crontab -e
```

2Ô∏è‚É£ Add this line to **run cleanup daily at midnight**:
```bash
0 0 * * * python3 /path/to/server.py cleanup_logs
```

‚úÖ **Now, logs are automatically cleaned every day!**  

---

# üì¶ Part 2: Automating Database Optimization  

As logs grow, **SQLite queries slow down**.  
We‚Äôll **vacuum the database** to keep it fast.

---

### üöÄ Step 1: Optimize Database Periodically  

Modify **`server.py`**:

```python
def optimize_database():
    """Optimizes the SQLite database to reclaim space."""
    cursor.execute("VACUUM")
    conn.commit()
    print("üöÄ Database optimized.")
```

‚úÖ **Now, our database runs efficiently!**  

---

### üöÄ Step 2: Automate Database Cleanup  

Add this cron job to **optimize the database weekly**:

```bash
0 3 * * 0 python3 /path/to/server.py optimize_database
```

‚úÖ **Now, the chatbot database stays fast and lightweight!**  

---

# üì° Part 3: Monitoring Chatbot Performance  

We‚Äôll monitor **response time** and send **alerts if the chatbot is slow**.

---

### üöÄ Step 1: Log Response Times  

Modify **`server.py`** to log **slow responses**:

```python
import smtplib

def send_alert(message):
    """Sends an email alert if chatbot is slow."""
    sender_email = "you@example.com"
    recipient_email = "admin@example.com"
    smtp_server = "smtp.example.com"
    smtp_port = 587
    smtp_username = "your_username"
    smtp_password = "your_password"

    subject = "‚ö†Ô∏è Chatbot Performance Alert"
    email_message = f"Subject: {subject}\n\n{message}"

    with smtplib.SMTP(smtp_server, smtp_port) as server:
        server.starttls()
        server.login(smtp_username, smtp_password)
        server.sendmail(sender_email, recipient_email, email_message)

@app.post("/chat")
def chat(query: str, api_key: str = Header(None)):
    """Logs response time and sends alerts if chatbot is slow."""
    
    if not api_key or not verify_api_key(api_key):
        raise HTTPException(status_code=401, detail="Invalid API Key")

    start_time = time.time()
    response = llm(query)["choices"][0]["text"]
    end_time = time.time()
    
    response_time = round(end_time - start_time, 2)

    if response_time > 5:
        send_alert(f"‚ö†Ô∏è Chatbot response time is {response_time}s! Investigate immediately.")

    return {"response": response, "response_time": response_time}
```

‚úÖ **Now, admins get alerts if chatbot response time is too high!**  

---

# üéõÔ∏è Part 4: Displaying System Status in Streamlit  

We‚Äôll show **system health** in the analytics dashboard.

---

### üöÄ Step 1: Fetch Chatbot Performance Data  

Modify **Streamlit UI (`app.py`)**:

```python
st.title("üìä Chatbot System Status")

# Fetch chatbot performance data
conn = sqlite3.connect("chat_logs.db")
cursor = conn.cursor()
cursor.execute("SELECT timestamp, response FROM chat_logs ORDER BY timestamp DESC LIMIT 5")
recent_responses = cursor.fetchall()

# Display response times
st.write("### ‚è≥ Recent Response Times")
for timestamp, response in recent_responses:
    if "Response Time:" in response:
        response_time = response.split("Response Time: ")[-1]
        st.write(f"üìÖ {timestamp} - ‚è≥ {response_time}")
```

‚úÖ **Now, admins can monitor chatbot performance in real-time!**  

---
<!-- 
# üéØ What We Automated in Part 10  

| Automation  | Status |
|------------|--------|
| **Auto-Cleanup of Old Logs (30 Days)** ‚úÖ Done |
| **Database Optimization (Weekly Cleanup)** ‚úÖ Done |
| **Monitoring Response Time & Sending Alerts** ‚úÖ Done |
| **System Health Dashboard in Streamlit** ‚úÖ Done |

---

## üîú Coming in Part 11: Advanced AI Features  

Now that our chatbot **runs smoothly**, we‚Äôll add **advanced AI features**.

### **üîπ In Part 11, we‚Äôll cover:**  
‚úÖ Using **memory-efficient embeddings** to improve search  
‚úÖ Adding **document summarization** for faster answers  
‚úÖ Implementing **multi-document reasoning** (cross-referencing PDFs)  

Stay tuned for **Part 11!** üöÄ  

---

## üìö References  
- [FastAPI Docs](https://fastapi.tiangolo.com/)  
- [SQLite Docs](https://www.sqlite.org/)  
- [Cron Job Guide](https://crontab.guru/)  
- [SMTP Email Alerts](https://docs.python.org/3/library/smtplib.html)  


Here‚Äôs **Part 11**, where we add **advanced AI features** like **memory-efficient embeddings, document summarization, and multi-document reasoning** to improve our AI chatbot. üöÄ  
--- 
-->

***

<!-- 
# On-Prem AI Chatbot for PDF Search ‚Äì Part 11: Advanced AI Features  

Welcome to **Part 11** of this series! Now that our chatbot is **optimized and runs smoothly**, it‚Äôs time to **make it smarter**.  

In **Part 10**, we:  
‚úÖ Automated **log cleanup & database maintenance**  
‚úÖ Added **performance monitoring & alerts**  
‚úÖ Built a **real-time chatbot health dashboard**  

Now, we‚Äôll improve the AI by adding:  
‚úÖ **Memory-efficient embeddings** (for better search)  
‚úÖ **Document summarization** (for faster answers)  
‚úÖ **Multi-document reasoning** (cross-referencing PDFs)  

---

## üöÄ What We‚Äôll Enhance in This Part  

| Feature  | Improvement |
|----------|------------|
| **Semantic Search** | Smaller, faster embeddings |
| **Summarization** | Generate concise answers |
| **Multi-Doc Reasoning** | Cross-reference multiple PDFs |

### **Recap of Our Tech Stack**
| Component             | Tool |
|-----------------------|---------------------|
| **FastAPI Backend**   | Enhancing Now üß† |
| **FAISS Vector Search** | Improving Speed üöÄ |
| **Llama 2 Chatbot**   | Adding Multi-Doc Reasoning üßê |

---

# üèéÔ∏è Part 1: Improving Semantic Search with Smaller Embeddings  

Currently, we use **384-dimensional embeddings** from `all-MiniLM-L6-v2`.  
This takes **a lot of memory** when storing thousands of PDFs in FAISS.  

### üöÄ Step 1: Switch to **Smaller, Faster Embeddings**  

Replace `all-MiniLM-L6-v2` with **a more compact model**.

```python
from sentence_transformers import SentenceTransformer

# Load a smaller, faster embedding model
model = SentenceTransformer("sentence-transformers/all-MiniLM-L12-v2")  # 12 layers instead of 6
```

‚úÖ **Now, embeddings use less memory but remain highly accurate!**  

---

### üöÄ Step 2: Reduce FAISS Index Size  

Modify **FAISS index to use PCA compression**.

```python
import faiss

DIMENSIONS = 384  # Original size
REDUCED_DIM = 256  # Compressed size

# Apply PCA compression
pca_matrix = faiss.PCAMatrix(DIMENSIONS, REDUCED_DIM)
index = faiss.IndexFlatL2(REDUCED_DIM)

# Train FAISS with PCA
pca_matrix.train(embeddings)
compressed_embeddings = pca_matrix.apply_py(embeddings)
index.add(compressed_embeddings)
```

‚úÖ **Now, FAISS uses 33% less memory but remains highly effective!**  

---

# üìÑ Part 2: Implementing Document Summarization  

Instead of returning **entire PDFs**, we‚Äôll **summarize key points**.  

---

### üöÄ Step 1: Install Summarization Model  

```bash
pip install transformers
```

---

### üöÄ Step 2: Summarize Documents Before Sending to Llama 2  

Modify **`server.py`**:

```python
from transformers import pipeline

# Load summarization model
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

def summarize_text(text):
    """Summarizes a long document."""
    return summarizer(text, max_length=200, min_length=50, do_sample=False)[0]["summary_text"]
```

‚úÖ **Now, documents are summarized before being fed into Llama 2!**  

---

### üöÄ Step 3: Integrate Summarization into RAG  

Modify **`search_pdfs_rag()`** to **summarize before retrieval**:

```python
def search_pdfs_rag(query, k=3):
    """Search PDFs and return summarized content."""
    
    es_results = search_pdfs(query)[:k]
    faiss_results = search_faiss(query, k)[:k]
    
    combined_results = set([summarize_text(r["_source"]["text"]) for r in es_results])
    combined_results.update([summarize_text(list(pdf_texts.values())[i]) for i in faiss_results[0]])
    
    return "\n\n".join(combined_results)
```

‚úÖ **Now, the chatbot only receives summarized PDFs, making responses more concise!**  

---

# üîó Part 3: Implementing Multi-Document Reasoning  

Right now, **Llama 2 treats each PDF separately**.  
We‚Äôll **combine multiple sources** and let the chatbot **cross-reference them**.

---

### üöÄ Step 1: Modify Prompt to Include Multiple PDFs  

Modify **`chat_with_pdfs()`**:

```python
def chat_with_pdfs(query):
    """Uses multiple PDFs as context for better answers."""
    
    context = search_pdfs_rag(query, k=5)  # Retrieve more docs for reasoning
    
    prompt = f"""
    You are an AI trained to answer questions using the following documents:
    
    {context}
    
    If multiple sources provide conflicting information, summarize the key differences.
    
    Question: {query}
    Answer:
    """
    
    response = llm(prompt)["choices"][0]["text"]
    return response
```

‚úÖ **Now, Llama 2 can reason across multiple PDFs and identify conflicts!**  

---

# üéØ What We Enhanced in Part 11  

| Feature  | Status |
|----------|--------|
| **Smaller, Faster Embeddings for FAISS** ‚úÖ Done |
| **Summarization Before Chatbot Processing** ‚úÖ Done |
| **Multi-Document Reasoning in Llama 2** ‚úÖ Done |

---

## üîú Coming in Part 12: Fine-Tuning AI for Domain-Specific Knowledge  

Now that our chatbot **thinks smarter**, we‚Äôll **fine-tune Llama 2** on **industry-specific PDFs**.

### **üîπ In Part 12, we‚Äôll cover:**  
‚úÖ Training **Llama 2 on company-specific data**  
‚úÖ Improving **accuracy in niche domains**  
‚úÖ Handling **ambiguous or conflicting answers**  

Stay tuned for **Part 12!** üöÄ  

---

## üìö References  
- [FAISS GitHub](https://github.com/facebookresearch/faiss)  
- [Sentence-Transformers](https://www.sbert.net/)  
- [Hugging Face Summarization Models](https://huggingface.co/models?pipeline_tag=summarization)  

---

‚úÖ **Next Up:** [On-Prem AI PDF Search - Part 12: Fine-Tuning AI for Domain-Specific Knowledge](#) (Coming Soon)  
```

---
 -->

 <!-- 
 Here‚Äôs **Part 12**, where we **fine-tune Llama 2 on domain-specific PDFs** to improve accuracy and adapt it for specialized knowledge. üöÄ  

---

```markdown
---
title: "On-Prem AI Chatbot for PDF Search ‚Äì Part 12: Fine-Tuning AI for Domain-Specific Knowledge"
description: "A simplified demonstration of a real-world project I built for work. In Part 12, we fine-tune Llama 2 on domain-specific PDFs to improve chatbot accuracy in specialized fields."
slug: "on-prem-ai-pdf-search-part-12"
date: 2019-12-18
image: "post/Articles/24.jpg"
categories: ["AI", "Machine Learning", "PDF Search", "Fine-Tuning"]
tags: ["AI", "Machine Learning", "LLM", "Llama 2", "FAISS", "Fine-Tuning", "Domain Adaptation"]
draft: false
weight: 530
---

# On-Prem AI Chatbot for PDF Search ‚Äì Part 12: Fine-Tuning AI for Domain-Specific Knowledge  

Welcome to **Part 12** of this series! Now that our chatbot **retrieves and summarizes PDFs effectively**, let‚Äôs make it **even smarter** by **fine-tuning Llama 2** on our **own dataset**.  

In **Part 11**, we:  
‚úÖ Improved **semantic search** with **smaller embeddings**  
‚úÖ Added **document summarization** for better responses  
‚úÖ Enabled **multi-document reasoning** for better accuracy  

Now, we‚Äôll:  
‚úÖ Train Llama 2 on **company-specific PDFs**  
‚úÖ Improve **accuracy for niche industry knowledge**  
‚úÖ Handle **ambiguous or conflicting answers**  

---

## üéØ Why Fine-Tune Llama 2?  

By default, **Llama 2 is a generalist**. Fine-tuning helps the model:  
‚úÖ **Learn domain-specific terminology** (e.g., legal, medical, finance)  
‚úÖ **Generate more accurate responses** for technical questions  
‚úÖ **Reduce hallucinations** (incorrect AI-generated answers)  

### **Recap of Our Tech Stack**
| Component             | Tool |
|-----------------------|---------------------|
| **FastAPI Backend**   | Fine-Tuning Now üéØ |
| **Llama 2 Chatbot**   | Training on Custom Data üèãÔ∏è‚Äç‚ôÇÔ∏è |
| **FAISS Vector Search** | Assisting with Context üîé |

---

# üìÇ Part 1: Preparing a Fine-Tuning Dataset  

We need to create **a structured dataset** from our **company PDFs**.  

---

### üöÄ Step 1: Convert PDFs into Training Data  

Modify **`prepare_data.py`**:

```python
import json
import fitz  # PyMuPDF

def extract_text_from_pdf(pdf_path):
    """Extracts text from a PDF using PyMuPDF."""
    doc = fitz.open(pdf_path)
    text = "\n".join([page.get_text("text") for page in doc])
    return text

training_data = []

pdf_files = ["docs/legal_contract.pdf", "docs/medical_guidelines.pdf"]

for pdf in pdf_files:
    text = extract_text_from_pdf(pdf)
    training_data.append({"input": f"Explain this document: {pdf}", "output": text})

# Save to JSONL format for fine-tuning
with open("training_data.jsonl", "w") as f:
    for item in training_data:
        f.write(json.dumps(item) + "\n")
```

‚úÖ **Now, we have a dataset ready for fine-tuning!**  

---

# üèãÔ∏è Part 2: Fine-Tuning Llama 2  

We‚Äôll use **Hugging Face‚Äôs `transformers` library** to fine-tune Llama 2.  

---

### üöÄ Step 1: Install Dependencies  

```bash
pip install torch transformers datasets peft accelerate
```

---

### üöÄ Step 2: Load Llama 2 for Fine-Tuning  

Modify **`fine_tune.py`**:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
import torch
import json

# Load base model & tokenizer
model_name = "meta-llama/Llama-2-7b-chat-hf"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Load training data
with open("training_data.jsonl", "r") as f:
    training_data = [json.loads(line) for line in f]

train_texts = [d["input"] for d in training_data]
train_labels = [d["output"] for d in training_data]

# Convert to tokenized format
train_encodings = tokenizer(train_texts, padding=True, truncation=True, return_tensors="pt")
label_encodings = tokenizer(train_labels, padding=True, truncation=True, return_tensors="pt")

# Fine-tuning settings
training_args = TrainingArguments(
    output_dir="./fine-tuned-llama",
    per_device_train_batch_size=2,
    num_train_epochs=3,
    save_strategy="epoch"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_encodings,
    eval_dataset=label_encodings
)

trainer.train()
model.save_pretrained("./fine-tuned-llama")
tokenizer.save_pretrained("./fine-tuned-llama")
```

‚úÖ **Now, Llama 2 is fine-tuned on our PDFs!**  

---

### üöÄ Step 3: Replace Default Model with Fine-Tuned One  

Modify **`server.py`**:

```python
from llama_cpp import Llama

# Load fine-tuned model
llm = Llama(model_path="fine-tuned-llama/llama-2-7b-chat.Q4_K_M.gguf")
```

‚úÖ **Now, the chatbot will generate domain-specific answers!**  

---

# üîó Part 3: Handling Conflicting Information  

Sometimes, **different documents have conflicting information**.  
We‚Äôll modify Llama 2 to **detect and explain inconsistencies**.

---

### üöÄ Step 1: Modify Prompt to Identify Conflicts  

Modify **`chat_with_pdfs()`**:

```python
def chat_with_pdfs(query):
    """Uses multiple PDFs and explains conflicting information."""
    
    context = search_pdfs_rag(query, k=5)
    
    prompt = f"""
    You are an AI trained to answer questions using the following documents:
    
    {context}
    
    If multiple sources provide conflicting information, summarize the key differences.

    Question: {query}
    Answer:
    """
    
    response = llm(prompt)["choices"][0]["text"]
    return response
```

‚úÖ **Now, the chatbot can highlight conflicting information!**  

---

# üéØ What We Improved in Part 12  

| Feature  | Status |
|----------|--------|
| **Training Llama 2 on Custom PDFs** ‚úÖ Done |
| **Fine-Tuning for Niche Knowledge** ‚úÖ Done |
| **Handling Conflicting Information** ‚úÖ Done |

---

## üîú Coming in Part 13: Multi-Language Support  

Now that our chatbot **is specialized for a domain**, we‚Äôll **add multi-language support**.

### **üîπ In Part 13, we‚Äôll cover:**  
‚úÖ Adding **multi-language embeddings** for FAISS  
‚úÖ Fine-tuning Llama 2 for **multilingual responses**  
‚úÖ Enabling **automatic language detection**  

Stay tuned for **Part 13!** üöÄ  

---

## üìö References  
- [Hugging Face Fine-Tuning Guide](https://huggingface.co/docs/transformers/training)  
- [FAISS GitHub](https://github.com/facebookresearch/faiss)  
- [Sentence-Transformers](https://www.sbert.net/)  

---

‚úÖ **Next Up:** [On-Prem AI PDF Search - Part 13: Multi-Language Support](#) (Coming Soon)  
```

---


 
  -->

<!-- -
Here‚Äôs **Part 13**, where we **add multi-language support** to our on-prem AI chatbot by enabling **multilingual embeddings, fine-tuning Llama 2 for multiple languages, and implementing automatic language detection**. üöÄ  

---

```markdown
---
title: "On-Prem AI Chatbot for PDF Search ‚Äì Part 13: Multi-Language Support"
description: "A simplified demonstration of a real-world project I built for work. In Part 13, we add multi-language support to our AI chatbot with multilingual embeddings, fine-tuned Llama 2, and automatic language detection."
slug: "on-prem-ai-pdf-search-part-13"
date: 2018-09-30
image: "post/Articles/32.jpg"
categories: ["AI", "Machine Learning", "PDF Search", "Multilingual AI"]
tags: ["AI", "Machine Learning", "LLM", "Llama 2", "FAISS", "Multilingual NLP"]
draft: false
weight: 550
---

# On-Prem AI Chatbot for PDF Search ‚Äì Part 13: Multi-Language Support  

Welcome to **Part 13** of this series! Now that our chatbot **is fine-tuned for domain-specific knowledge**, we‚Äôll **expand its capabilities** by adding **multi-language support**.  

In **Part 12**, we:  
‚úÖ Fine-tuned **Llama 2 on custom PDFs**  
‚úÖ Improved **accuracy in niche fields**  
‚úÖ Enabled **multi-document reasoning**  

Now, we‚Äôll:  
‚úÖ Add **multi-language embeddings** for FAISS  
‚úÖ Fine-tune **Llama 2 for multilingual responses**  
‚úÖ Enable **automatic language detection**  

---

## üåç Why Multi-Language Support?  

Currently, our chatbot **only understands English**. Adding multilingual support allows:  
‚úÖ **Global users** to interact in their native languages  
‚úÖ **Multilingual document search** (e.g., legal docs in French, tech manuals in German)  
‚úÖ **More inclusive AI** with broader accessibility  

### **Recap of Our Tech Stack**
| Component             | Tool |
|-----------------------|---------------------|
| **FastAPI Backend**   | Enhancing Now üåç |
| **FAISS Vector Search** | Adding Multi-Language üîé |
| **Llama 2 Chatbot**   | Training for Multiple Languages üèãÔ∏è‚Äç‚ôÇÔ∏è |

---

# üèéÔ∏è Part 1: Enabling Multilingual Embeddings  

FAISS currently **stores English-only embeddings**.  
We‚Äôll switch to **a multilingual model** that supports **100+ languages**.

---

### üöÄ Step 1: Install a Multilingual Embedding Model  

Modify **FAISS setup** to use **`paraphrase-multilingual-MiniLM-L12-v2`**, a **small but powerful** multilingual model.

```bash
pip install sentence-transformers
```

Modify **`server.py`**:

```python
from sentence_transformers import SentenceTransformer

# Load a multilingual embedding model
model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
```

‚úÖ **Now, FAISS supports embeddings in 100+ languages!**  

---

### üöÄ Step 2: Reindex Documents with Multilingual Embeddings  

Modify **document indexing** to store **new embeddings**.

```python
import numpy as np
import faiss

DIMENSIONS = 384  # Model output size

# Load existing documents
pdf_texts = {"contract_fr.pdf": "Contrat de travail en fran√ßais.", "manual_de.pdf": "Technisches Handbuch auf Deutsch."}

# Generate embeddings
embeddings = np.array([model.encode(text) for text in pdf_texts.values()])

# Create FAISS index
index = faiss.IndexFlatL2(DIMENSIONS)
index.add(embeddings)
```

‚úÖ **Now, FAISS can store and search non-English documents!**  

---

# üß† Part 2: Fine-Tuning Llama 2 for Multiple Languages  

By default, **Llama 2 is strongest in English**.  
We‚Äôll **fine-tune it** on **multilingual text**.

---

### üöÄ Step 1: Download Multilingual Dataset  

We need **sample conversations** in multiple languages.  
Download the **OpenAssistant multilingual dataset**:

```bash
wget https://huggingface.co/datasets/OpenAssistant/oasst1/resolve/main/oasst1_multilingual.jsonl
```

---

### üöÄ Step 2: Train Llama 2 on Multilingual Data  

Modify **`fine_tune.py`**:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
import torch
import json

# Load base model
model_name = "meta-llama/Llama-2-7b-chat-hf"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Load multilingual training data
with open("oasst1_multilingual.jsonl", "r") as f:
    training_data = [json.loads(line) for line in f]

train_texts = [d["input"] for d in training_data]
train_labels = [d["output"] for d in training_data]

# Tokenize data
train_encodings = tokenizer(train_texts, padding=True, truncation=True, return_tensors="pt")
label_encodings = tokenizer(train_labels, padding=True, truncation=True, return_tensors="pt")

# Fine-tuning settings
training_args = TrainingArguments(
    output_dir="./fine-tuned-llama-multilingual",
    per_device_train_batch_size=2,
    num_train_epochs=3,
    save_strategy="epoch"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_encodings,
    eval_dataset=label_encodings
)

trainer.train()
model.save_pretrained("./fine-tuned-llama-multilingual")
tokenizer.save_pretrained("./fine-tuned-llama-multilingual")
```

‚úÖ **Now, Llama 2 understands multiple languages!**  

---

# üåç Part 3: Automatic Language Detection  

Instead of asking users to select a language, we‚Äôll **automatically detect** it.

---

### üöÄ Step 1: Install Language Detection  

```bash
pip install langdetect
```

Modify **`server.py`**:

```python
from langdetect import detect

def detect_language(text):
    """Detects the language of a given text."""
    return detect(text)

# Example usage
query = "¬øC√≥mo funciona la inteligencia artificial?"
print(detect_language(query))  # Output: "es" (Spanish)
```

‚úÖ **Now, we can detect user language before responding!**  

---

### üöÄ Step 2: Choose the Correct Llama 2 Model  

Modify **`chat_with_pdfs()`**:

```python
from llama_cpp import Llama

# Load both English and multilingual models
llm_en = Llama(model_path="fine-tuned-llama/llama-2-7b-chat.Q4_K_M.gguf")
llm_multi = Llama(model_path="fine-tuned-llama-multilingual/llama-2-7b-chat.Q4_K_M.gguf")

def chat_with_pdfs(query):
    """Detects language and selects the correct model."""
    
    lang = detect_language(query)

    if lang in ["en", "fr", "de", "es"]:  # Example supported languages
        llm = llm_multi  # Use multilingual model
    else:
        llm = llm_en  # Default to English model
    
    response = llm(query)["choices"][0]["text"]
    
    return response
```

‚úÖ **Now, the chatbot automatically responds in the user‚Äôs language!**  

---

# üéØ What We Improved in Part 13  

| Feature  | Status |
|----------|--------|
| **Multilingual Embeddings for FAISS** ‚úÖ Done |
| **Fine-Tuned Llama 2 for Multiple Languages** ‚úÖ Done |
| **Automatic Language Detection** ‚úÖ Done |

---

## üîú Coming in Part 14: Voice Input & Text-to-Speech  

Now that our chatbot **understands multiple languages**, we‚Äôll add **voice capabilities**.

### **üîπ In Part 14, we‚Äôll cover:**  
‚úÖ Adding **speech-to-text** (voice queries)  
‚úÖ Implementing **text-to-speech** for responses  
‚úÖ Enhancing **accessibility for visually impaired users**  

Stay tuned for **Part 14!** üöÄ  

---

## üìö References  
- [FAISS GitHub](https://github.com/facebookresearch/faiss)  
- [Sentence-Transformers](https://www.sbert.net/)  
- [Hugging Face Multilingual Dataset](https://huggingface.co/datasets/OpenAssistant/oasst1)  
- [LangDetect Docs](https://pypi.org/project/langdetect/)  

---

‚úÖ **Next Up:** [On-Prem AI PDF Search - Part 14: Voice Input & Text-to-Speech](#) (Coming Soon)  
```

---


->
